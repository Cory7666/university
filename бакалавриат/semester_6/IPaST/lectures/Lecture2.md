# Лекция №2. Энтропия дискретных источников связи

Энтропия - степень неопределённости.

Информация - новое сведение, получаемое в ходе снятия неопределённости.

Автор теории информации - ... .

Требования к энтропии:
1. линейное возрастание;
2. аддитивность;
3. наличие функции вероятности.

Формула:

$H(x) = - \sum_{i=1}^{n} {p(x_i) \log_2 {p(x_i)} }$; $\sum p(x_i) = 1$.

# Пример:
1. Буква алфавита равномерна ($p = \frac {1} {32}$).
   
   $H(x) = - \log_2 \frac {1} {32} = -5$.

2. Буква не равномерна:

    $-0.064 \log_2 {0.064} - 0.015 \log_2 {0.015} - ... = ~-4.42$

Вывод: неравномерность распределения вероятности снижает энтропию.

$
\begin{cases}
H(x_i) = -\log {p(x_i)} - \text{частичная энтропия на 1 символ} \\
H(X) = - \sum_{i=1}^{n} {p(x_i) H(x_i)} = - \sum_{i=1}^{n} {p(x_i) \log {p(x_i)}} - \text{средняя энтропия источника} 
\end{cases}
$

$
\begin{cases}
H(x_i / y_i) = - \log{p(x_i / y_i)} - \text{частичная условная энтропия} \\
H(X / y_i) = - \sum_{i=1}^{n} {p(x_i / y_i) \log {p(x_i / y_i)}} - \text{средняя условная энтропия} \\
H(X / Y) = - \sum_i { \sum_j { p(x_i / y_j) \log{p(x_i / y_j)} } }
\end{cases}
$
