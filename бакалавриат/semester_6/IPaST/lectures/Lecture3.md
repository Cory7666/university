# Лекция 3. Информация, содержащаяся в случайных событиях

Дифференциальная энтопия непрерывного источника информации.

$h(U) = - \int_{-\infty}^{+\infty} {f(u) log(p(u))du}$,
где $f(u)$ - область распределения непрерывной случайной величины.

Дифференциальная условная энтропия непрерывного источника:
$h(\frac {U} {V}) \equiv h_V(U)$

Плотность распределения непрерывной случайной величины:
$frac f(U) \equiv p(U)$

Если протность распределения == 1, то на интервале $\beta - \alpha$ высота равна $\frac 1 {\beta - \alpha}$.

Свойства дифференциальной энтропии:
1. Масштабирование по амплитуде случайной величины изменяет дифференциальную энтропию на $\log(k)$, где k - коэффициент масштабирования.
2. Постоянная добавка к случайной величине не меняет дифференциальную энтропию.
3. **При ограничении на мощность** (дисперсия) нормальное распределение будет обладать большей энтропией нежели равномерное распределение.
4. Совместная дифференциальная энтропия двух случайных величин вычисляется как:
$h(U, V) = - \int_{-\infty}^{+\infty} {f(U, V) log(f(U, V))}$.

## Определение информации

Информация -
1. новое сведение.
2. мера снятия (уменьшения) энтропии.

Информация относительно источника: $I(V)$;

источник: $H(V_i)$;

приёмник: $H(V_i/U_j)$.

То: $I(V_i) = H(V_i) - H(V_i/U_j)$.

Если $H(V_i) = -log(p(V_i))$, $H(V_i/U_j) = -log(p(V_i/U_j))$, то:
$I(V_i) = log(\frac {p(V_i / U_j)} {p(V_i)})$.

Это определения информации относительно частной энтропии и частного количества информации $i$ и $j$.

$I(V_i U_j) = I(U_j V_i) = \log { \frac {p(V_i U_j)} {p(V_i) p(U_j)} }$

На основе частной информации можно получить среднее количество информации (по правилу вычисления математического ожидания):

$I(V) = H(V) - H(V/U)$

## Свойства количества информации
1. $I(V) >= 0$.
2. При отсутствии статистической связи между случайными величинами $I(VU) = 0$.
3. (Свойство симметрии) $I(VU) = I(UV)$.
4. Если принимается сигнал без помех ($H(V/U) = 0$), то $I(V) = H(V)$. Иначе говоря: максимальное количество информации можно получить только в том случае, если остаточная энтропия равна нулю.
